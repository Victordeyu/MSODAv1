{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import scipy.linalg as la\n",
    "from sklearn.svm import SVC\n",
    "# from JDIP import JDIP\n",
    "# from MSJDIP import MSJDIP\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import scale,LabelEncoder\n",
    "from os import path as osp\n",
    "import autograd.numpy as anp\n",
    "import autograd.numpy.linalg as alina\n",
    "import pymanopt as pm\n",
    "import pymanopt.manifolds as manifolds\n",
    "import pymanopt.optimizers as optimizers\n",
    "from sklearn.metrics import pairwise_distances\n",
    "import autograd.numpy as anp\n",
    "import autograd.numpy.linalg as alina\n",
    "import numpy as np\n",
    "import numpy.linalg as nlina\n",
    "import pandas as pd\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.preprocessing import scale\n",
    "import sys\n",
    "\n",
    "\n",
    "def readDataMS(root,scs,tg,Cs_end,Cu_start,fn='fea',postfix=''): \n",
    "    '''\n",
    "    root: 数据集路径,eg 'OSDA-2/Office-31_Alex/Data_office31'\n",
    "    scs: List of source domain name, eg ['dslr','webcam']\n",
    "    tg: name of target domain name, eg 'amazon'\n",
    "    Cs_end: Known end\n",
    "    Cu_start: Unknown start\n",
    "    fn: 在mat文件中feature列名 \n",
    "    postfix: 每个数据集文件的后缀名\n",
    "\n",
    "    return: \n",
    "        Xs: List of numpy array\n",
    "        Xt: Numpy array\n",
    "        l: numpy array of label of the sample\n",
    "\n",
    "    '''\n",
    "    Xs,ys,l=[],[],[]\n",
    "    li=0\n",
    "    for sc in scs:\n",
    "        data = sio.loadmat(osp.join(root ,sc + postfix+'.mat'))# source domain \n",
    "        Xsi,ysi = data[fn].astype(np.float64),data['labels'].ravel()\n",
    "        ysi = LabelEncoder().fit(ysi).transform(ysi).astype(np.float64)\n",
    "        Xsi = Xsi / la.norm(Xsi,axis=1,keepdims=True)\n",
    "        Xsn=Xsi[ysi[:]<Cs_end,:]\n",
    "        ysn=ysi[ysi[:]<Cs_end]#筛选出已知类\n",
    "        Xs.append(Xsn)\n",
    "        ys.append(ysn)\n",
    "        l=np.hstack((np.array(l),np.full((Xsn.shape[0],),li)))\n",
    "        li+=1\n",
    "\n",
    "    data = sio.loadmat(osp.join(root , tg + postfix+'.mat'))# target domain \n",
    "    Xt,yt = data[fn].astype(np.float64),data['labels'].ravel()\n",
    "    yt = LabelEncoder().fit(yt).transform(yt).astype(np.float64)\n",
    "    C=len(np.unique(yt))\n",
    "    # index_unknwon=yt[yt[:]>Cs_end and yt[:]<Cu_start]\n",
    "    Xt=np.vstack((Xt[yt[:]<Cs_end,:],Xt[yt[:]>=Cu_start,:]))#筛选已知类和未知类\n",
    "    yt=np.hstack((yt[yt[:]<Cs_end],yt[yt[:]>=Cu_start]))\n",
    "\n",
    "    l=np.hstack((np.array(l),np.full((Xt.shape[0],),li)))\n",
    "    \n",
    "    Xt = Xt / la.norm(Xt,axis=1,keepdims=True)\n",
    "\n",
    "    Cs = Cs_end\n",
    "    Cu = C - Cu_start\n",
    "\n",
    "    return Xs,ys,Xt,yt,l,Cs,Cu\n",
    "\n",
    "def compute_one_unknown(Ytu, Ytu_pseudo, Cs):\n",
    "    \"\"\"\n",
    "    :param Ytu: 实际未知类的标签,全部为未知类\n",
    "    :param Ytu_pseudo: 实际未知类的伪标签,可能混合已知类\n",
    "    :param Cs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 计算预测未知类中真实未知类的准确率\n",
    "    one_unknown = Ytu_pseudo[Ytu_pseudo >= Cs].shape[0] / Ytu.shape[0]\n",
    "    # Ytu2=Ytu.copy()\n",
    "    # Ytu2[Ytu2>=Cs]=1\n",
    "    # one_unknown = (Ytu_pseudo[Ytu2 == Cs]==Cs).sum() / Ytu.shape[0]\n",
    "\n",
    "    return one_unknown\n",
    "\n",
    "def compute_acc_known(Ytk, Ytk_pseudo, Cs):\n",
    "    \"\"\"\n",
    "    :param Ytk: 实际已知类的标签\n",
    "    :param Ytk_pseudo: 实际已知类的伪标签\n",
    "    :param Cs:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # 计算已知类的平均准确率\n",
    "    acc_known = 0\n",
    "    for c in range(Cs):  # 计算已知类的分类准确率\n",
    "        known_i = np.sum(Ytk_pseudo[Ytk == c] == Ytk[Ytk == c]) / Ytk[Ytk == c].shape[0]\n",
    "        acc_known = acc_known + known_i\n",
    "\n",
    "    acc_known = acc_known / Cs\n",
    "\n",
    "    return acc_known\n",
    "\n",
    "# def compute_acc(Y, Y_pseudo, Cs):\n",
    "#     acc_known = 0\n",
    "#     acc_unknown = 0\n",
    "#     known_i = 0\n",
    "#     unknown_i = 0\n",
    "#     mtk = Y[Y < Cs].shape\n",
    "#     mtu = Y[Y >= Cs].shape\n",
    "#     mt = Y.shape\n",
    "#     # 计算已知类的分类准确率\n",
    "#     for c in range(Cs):  \n",
    "#         known_i += np.sum(Y_pseudo[Y == c] == Y[Y == c]) \n",
    "#     acc_known = known_i/mtk\n",
    "#     # 计算未知类的分类准确率\n",
    "#     unknown_i = np.sum(Y_pseudo[Y >= Cs ] == Cs)\n",
    "#     acc_unknown = unknown_i/mtu\n",
    "#     # 计算总体分类准确率\n",
    "#     acc = (unknown_i+known_i)/mt\n",
    "#     print('use this')\n",
    "#     return acc,acc_unknown,acc_known\n",
    "\n",
    "def compute_acc(Y, Y_pseudo, Cs):\n",
    "    acc_os_all = 0\n",
    "    acc_os_k = 0\n",
    "    os = 0\n",
    "    os_1 = 0 \n",
    "    # print(Y)\n",
    "    # print(Y_pseudo)\n",
    "    # 计算已知类的分类准确率\n",
    "    for c in range(Cs):\n",
    "        os_1 = np.sum(Y_pseudo[Y == c] == Y[Y == c]) /Y[Y == c].shape\n",
    "        acc_os_k += os_1\n",
    "    acc_os_k = acc_os_k/Cs\n",
    "    print(acc_os_k)\n",
    "    # 计算未知类的分类准确率\n",
    "    for c in range(Cs):\n",
    "        os = np.sum(Y_pseudo[Y == c] == Y[Y == c]) /Y[Y == c].shape\n",
    "        acc_os_all += os \n",
    "    os_unk = np.sum(Y_pseudo[Y >= Cs ] == Cs)/Y[Y >= Cs ].shape\n",
    "    acc_os_all += os_unk\n",
    "    acc_os_all = acc_os_all/(Cs+1)\n",
    "    HOS=2*(acc_os_k*os_unk)/(acc_os_k+os_unk)\n",
    "    return acc_os_all,os_unk,acc_os_k,HOS\n",
    "\n",
    "def cla_Svc(Xs, Xt, Ys):\n",
    "    # 2.SVM分类模型\n",
    "    model_cla = LinearSVC(dual=False)  # dual决定无法收敛时取消默认1000的迭代次数\n",
    "    model_cla.fit(Xs, Ys)\n",
    "    conf_matrix = model_cla.decision_function(Xt)\n",
    "    conf_label = conf_matrix.argmax(axis=1)  # 每个样本最大置信度的索引,即类\n",
    "    conf_vec = np.max(conf_matrix, axis=1)  # 每个样本最大置信度\n",
    "\n",
    "    return conf_label, conf_vec\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def cost_manifold(Ws,Wt,l, Cs, Xs, Ys ,Xt,Yt, Maxiter, dimension, truncated_param=1e-8):\n",
    "    # FX=np.vstack(FX,Xt)\n",
    "    # y=np.vstack(ys,Yt_pseudo)\n",
    "    # print(Ws,Wt,l,Cs,Xs)\n",
    "    dim = Xs.shape[1]\n",
    "    manifold = manifolds.Product([manifolds.Stiefel(dim, dimension), manifolds.Stiefel(dim, dimension)])\n",
    "\n",
    "    # 2.切分Xt已知类和未知类\n",
    "    Xtk = Xt[Yt < Cs, :]\n",
    "    Ytk = Yt[Yt < Cs]\n",
    "    # print(Ys.shape,Yt.shape)\n",
    "    lk=l[np.hstack((Ys,Yt))<Cs]\n",
    "    # 融合源域和目标域\n",
    "    Yk_all=np.hstack((Ys,Ytk))\n",
    "    Xk_all=np.vstack((Xs,Xtk))\n",
    "\n",
    "    @pm.function.autograd(manifold)\n",
    "    def cost(Ws,Wt):\n",
    "        # 1.更新Xs\n",
    "        Xs_W = anp.dot(Xs, Ws)\n",
    "        Xtk_W = anp.dot(Xtk, Wt)\n",
    "        # print(lk,Ytk)\n",
    "        X_all_W=anp.vstack((Xs_W,Xtk_W))\n",
    "\n",
    "        # # print(type(Xall_W[0]),type(Xall_W))\n",
    "        known_dist = pairwise_distances(Xk_all,Xk_all, 'euclidean')**2\n",
    "        known_sigma = np.median(known_dist[known_dist != 0])\n",
    "        # # print(\"sigma\",known_sigma)\n",
    "\n",
    "        known_rcs=RMI_np(X_all_W,Yk_all,lk,sigma=known_sigma,alpha=0)\n",
    "        # 没有*2开根号和外部参数\n",
    "        # print('rcs:',type(known_rcs),known_rcs)\n",
    "        if known_rcs<0:\n",
    "            known_rcs=0\n",
    "\n",
    "        return known_rcs\n",
    "\n",
    "    problem = pm.Problem(manifold=manifold, cost=cost)\n",
    "    optimizer = optimizers.SteepestDescent(max_iterations=Maxiter)\n",
    "    if Ws is None and Wt is None:\n",
    "        W = optimizer.run(problem)\n",
    "    else:\n",
    "        W = optimizer.run(problem, initial_point=[Ws[:, :dimension], Wt[:, :dimension]])\n",
    "    return W\n",
    "\n",
    "def RMI_np(FX,y,l,sigma=0.8,alpha=0.5,lamda=1e-2):\n",
    "    '''\n",
    "    FX: numpy array of feature. eg: vstack(Xs)\n",
    "    y: numpy array of labels\n",
    "    l: numpy array of domain labels\n",
    "    \n",
    "    return -->float RCS Divergence of FX,y,l\n",
    "    '''\n",
    "    m=FX.shape[0]\n",
    "    \n",
    "    Deltay=(y[:,None]==y).astype(float)\n",
    "    Deltal=(l[:,None]==l).astype(float)\n",
    "    FX_norm=anp.sum(FX**2,axis=-1)#这里由于做了归一化 所以始终都等于1 后面要考虑下要不要改\n",
    "    # print(FX_norm,FX_norm.shape)\n",
    "    # print('1',FX_norm[:,None],FX_norm[None,:])\n",
    "    # print(FX_norm[:,None].shape,FX_norm[None,:].shape)\n",
    "    A=-(FX_norm[:,None] + FX_norm[None,:])\n",
    "    # print(\"A\",A.shape)\n",
    "    B=2 * anp.dot(FX, FX.T)\n",
    "    # print(\"B\",B.shape)\n",
    "    K=anp.exp(-(FX_norm[:,None] + FX_norm[None,:] - 2 * anp.dot(FX, FX.T)) / sigma) * Deltay\n",
    "    # print('K is',K.shape)\n",
    "    P = K * Deltal\n",
    "    # print(\"P is\",P.shape)\n",
    "    H = ((1.0 - alpha) / m**2) * anp.dot(K,K) * anp.dot(Deltal,Deltal) + 1.0 * alpha / m * anp.dot(P,P)\n",
    "    h = anp.mean(P,axis=0)\n",
    "    h=h.reshape((h.shape[0],))#对齐一下向量\n",
    "\n",
    "    # theta = anp.matrix(H + lamda * anp.eye(m)).I.dot(h)\n",
    "    theta=alina.solve(H + lamda * anp.eye(m),h)\n",
    "    \n",
    "    D = 2 * anp.dot(h.T,theta)-anp.dot(theta.T,anp.dot(H,theta)) - 1\n",
    "    # print(D,type(D))\n",
    "\n",
    "    # print(H.shape,h.shape,theta.shape)\n",
    "    return D\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def model_loss(Xs, Ys, Xt, Yt_pseudo, l,Cs, sigma, eta, gamma_tk, gamma_tu, gamma_s):\n",
    "    # 数据初始化\n",
    "    ms = Xs.shape[0]\n",
    "    mt = Xt.shape[0]\n",
    "    data_X = anp.vstack((Xs, Xt))\n",
    "    l_unique,l_count=np.unique(l,return_counts=True)\n",
    "    print('ms:{},mt:{}'.format(ms,mt))\n",
    "    print('lu:',l_unique,'lc',l_count)\n",
    "    # print(len(Ys),Ys)\n",
    "\n",
    "    # 初始化源域已知类的标签矩阵\n",
    "    Y0 = np.zeros([Cs + 1, ms + mt])\n",
    "    for index in range(ms):\n",
    "        # print(Ys[index],type(Ys[index]))\n",
    "        Y0[int(Ys[index]), index] = 1\n",
    "\n",
    "    # 初始化被视为未知类的标签矩阵\n",
    "    Yu = np.zeros([Cs + 1, ms + mt])\n",
    "    Yu[Cs, :] = 1\n",
    "\n",
    "    # 初始化W,V1,V2矩阵\n",
    "    W = np.zeros([ms + mt, ms + mt])\n",
    "    w1, w2 = np.diag_indices_from(W)\n",
    "    # W[w1[:ms], w2[:ms]] = np.sqrt(1 / ms)\n",
    "    \n",
    "    current_index=0\n",
    "    for i in range(len(l_unique)-1):#最后一个域是目标域 不纳入考虑\n",
    "        start=current_index\n",
    "        end=current_index+l_count[i]\n",
    "        W[w1[start:end], w2[start:end]] = np.sqrt(1 / l_count[i])\n",
    "        current_index=end\n",
    "        # print(l_count[i])\n",
    "    print('current_index',current_index)\n",
    "    # print(W)\n",
    "    \n",
    "    mtk = Yt_pseudo[Yt_pseudo < Cs].shape[0]\n",
    "    mtu = Yt_pseudo[Yt_pseudo >= Cs].shape[0]\n",
    "    V1 = np.zeros([ms + mt, ms + mt])\n",
    "    v11, v21 = np.diag_indices_from(V1)\n",
    "    V1[v11[ms:][Yt_pseudo < Cs], v21[ms:][Yt_pseudo < Cs]] = np.sqrt(1 / mtk)\n",
    "    V2 = np.zeros([ms + mt, ms + mt])\n",
    "    v12, v22 = np.diag_indices_from(V2)\n",
    "    V2[v12[ms:][Yt_pseudo >= Cs], v22[ms:][Yt_pseudo >= Cs]] = np.sqrt(1 / mtu)\n",
    "\n",
    "    # 计算特征x的核矩阵K\n",
    "    data_norm = np.sum(data_X ** 2, axis=-1)\n",
    "    pair_dist = data_norm[:, None] + data_norm[None, :] - 2 * np.dot(data_X, data_X.T)\n",
    "    K = np.exp(- pair_dist / sigma)\n",
    "\n",
    "    # 计算alpha\n",
    "    V_sub_W = gamma_tk * np.dot(V1, V1) + gamma_tu * np.dot(V2, V2) - gamma_s * np.dot(W, W)\n",
    "    alpha_l = np.dot(np.dot(W, W), K) + np.dot(V_sub_W, K) + eta * np.eye(ms + mt)\n",
    "    alpha_r = np.dot(np.dot(W, W), Y0.T) + np.dot(V_sub_W, Yu.T)\n",
    "    alpha = nlina.solve(alpha_l, alpha_r)\n",
    "    predict_alpha = np.dot(alpha.T, K)\n",
    "\n",
    "    # 计算源域损失\n",
    "    known_fro = nlina.norm(np.dot(Y0 - predict_alpha, W), ord='fro')\n",
    "    known_loss = known_fro * known_fro\n",
    "\n",
    "    # 计算被视为未知类损失\n",
    "    target_tk_fro = nlina.norm(np.dot(Yu - predict_alpha, V1), ord='fro')\n",
    "    unknown_target_tk = gamma_tk * target_tk_fro * target_tk_fro\n",
    "    target_tu_fro = nlina.norm(np.dot(Yu - predict_alpha, V2), ord='fro')\n",
    "    unknown_target_tu = gamma_tu * target_tu_fro * target_tu_fro\n",
    "    source_fro = nlina.norm(np.dot(Yu - predict_alpha, W), ord='fro')\n",
    "    unknown_source = gamma_s * source_fro * source_fro\n",
    "    unknown_loss = unknown_target_tk + unknown_target_tu - unknown_source\n",
    "\n",
    "    # 计算正则化项\n",
    "    regular_norm = eta * np.sum(np.dot(predict_alpha, alpha).diagonal())\n",
    "\n",
    "    loss = known_loss + unknown_loss + regular_norm\n",
    "    print('loss is', format(loss, '.2f'), '| known_loss is', format(known_loss, '.2f'), '| unknown_loss is',\n",
    "          format(unknown_loss, '.2f'), '(', format(unknown_target_tk, '.2f'), '+', format(unknown_target_tu, '.2f'), '-', format(unknown_source, '.2f'), ')')\n",
    "    return loss, predict_alpha,alpha\n",
    "\n",
    "def make_print_to_file(filename=\"Default.log\",path='./'):\n",
    "    '''\n",
    "    path: it is a path for save your log about fuction print\n",
    "    example:\n",
    "    use  make_print_to_file()   and the   all the information of funtion print , will be write in to a log file\n",
    "    :return:\n",
    "    '''\n",
    "    import sys\n",
    "    import os\n",
    "    import sys\n",
    "    import datetime\n",
    " \n",
    "    class Logger(object):\n",
    "        def __init__(self, filename=filename, path=\"./\"):\n",
    "            self.terminal = sys.stdout\n",
    "            self.path= os.path.join(path, filename)\n",
    "            # if not os.path.exists(self.path):\n",
    "                \n",
    "            self.log = open(self.path, \"a\", encoding='utf8',)\n",
    "            print(\"save:\", os.path.join(self.path, filename))\n",
    " \n",
    "        def write(self, message):\n",
    "            self.terminal.write(message)\n",
    "            self.log.write(message)\n",
    " \n",
    "        def flush(self):\n",
    "            pass\n",
    "    \n",
    "    print(filename)\n",
    "    fileName = filename+datetime.datetime.now().strftime('day'+'%Y_%m_%d_%H_%M')\n",
    "    sys.stdout = Logger(fileName + '.log', path=path)\n",
    " \n",
    "    #############################################################\n",
    "    # 这里输出之后的所有的输出的print 内容即将写入日志\n",
    "    #############################################################\n",
    "    print(fileName.center(60,'*'))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Office31_Mlti_2amazon__proTest\n",
      "save: logs\\Office31_Mlti_2amazon__proTestday2023_09_22_11_08.log\\Office31_Mlti_2amazon__proTestday2023_09_22_11_08.log\n",
      "*****Office31_Mlti_2amazon__proTestday2023_09_22_11_08******\n",
      "DataSet:Office-31_Alex\\Data_office31 \n",
      "Target Domain:amazon\n"
     ]
    }
   ],
   "source": [
    "scs=['webcam','dslr']\n",
    "tg='amazon'\n",
    "domain_num=3\n",
    "root=osp.join('Office-31_Alex','Data_office31')\n",
    "dataSet='Office31'\n",
    "\n",
    "eta=0.001\n",
    "gamma_tk=0.2\n",
    "gamma_tu=0.7\n",
    "gamma_s=0.4\n",
    "conf=-0.25\n",
    "u_conf=-0.6\n",
    "make_print_to_file(filename='{}_Mlti_2{}__proTest'.format(dataSet,tg),path='logs')\n",
    "print(\"DataSet:{}\".format(root),\"\\nTarget Domain:{}\".format(tg))\n",
    "\n",
    "Xs,ys,Xt,yt,l,Cs,Cu=readDataMS(root,scs,tg,20,21,fn='fts',postfix='_Al7')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing...\n",
      "Iteration    Cost                       Gradient norm     \n",
      "---------    -----------------------    --------------    \n",
      " 1           +8.7318023838268477e-02    5.46371645e-02    \n",
      " 2           +6.6494835159777832e-02    3.99715868e-02    \n",
      " 3           +5.8394915936603287e-02    2.56615088e-02    \n",
      " 4           +5.5260477682255171e-02    2.16374202e-02    \n",
      " 5           +5.3928010843346419e-02    2.56800907e-02    \n",
      " 6           +5.1891595051472894e-02    1.52343187e-02    \n",
      " 7           +5.0878163352706895e-02    1.20409420e-02    \n",
      " 8           +5.0047507851576523e-02    1.54888809e-02    \n",
      " 9           +4.9871422651605712e-02    2.27418836e-02    \n",
      "10           +4.9247566241556573e-02    1.76416273e-02    \n",
      "11           +4.8298841808857684e-02    1.13247482e-02    \n",
      "12           +4.7903694826707977e-02    1.70376766e-02    \n",
      "13           +4.6996232282844552e-02    7.27006393e-03    \n",
      "14           +4.3794630644041987e-02    9.99392966e-03    \n",
      "15           +4.3534948590388423e-02    1.65720639e-02    \n",
      "16           +4.2813742229577567e-02    7.78141286e-03    \n",
      "17           +4.2339989864266681e-02    1.60472188e-02    \n",
      "18           +4.1582661328914527e-02    8.61851748e-03    \n",
      "19           +4.1567765466560047e-02    1.85914693e-02    \n",
      "20           +4.1509074167592308e-02    1.80365683e-02    \n",
      "21           +4.1288942907853343e-02    1.58086265e-02    \n",
      "22           +4.0668228522181327e-02    7.38887244e-03    \n",
      "23           +4.0585659307523336e-02    1.68809096e-02    \n",
      "24           +4.0288585671553356e-02    1.35267764e-02    \n",
      "25           +3.9733870142307559e-02    5.98411205e-03    \n",
      "26           +3.9551813953635939e-02    1.60569006e-02    \n",
      "27           +3.9004208870686563e-02    8.43165647e-03    \n",
      "28           +3.8739941667726097e-02    5.70525202e-03    \n",
      "29           +3.8508351377688488e-02    8.45996517e-03    \n",
      "30           +3.8419557374150815e-02    1.15783418e-02    \n",
      "31           +3.8142088986734990e-02    6.82688178e-03    \n",
      "32           +3.7967219104903194e-02    8.02388490e-03    \n",
      "33           +3.7793622722758435e-02    8.53131540e-03    \n",
      "34           +3.7581880795298339e-02    7.46324555e-03    \n",
      "35           +3.7372429526357998e-02    5.03430076e-03    \n",
      "36           +3.7173839415130416e-02    7.47537562e-03    \n",
      "37           +3.7148748987825497e-02    1.16267438e-02    \n",
      "38           +3.7054705944560506e-02    1.01776990e-02    \n",
      "39           +3.6789772144922850e-02    4.88427107e-03    \n",
      "40           +3.6641138426344311e-02    1.00323817e-02    \n",
      "41           +3.6333770233410556e-02    4.09843681e-03    \n",
      "42           +3.5991219763829907e-02    1.71733447e-02    \n",
      "43           +3.5206047705062193e-02    4.05821635e-03    \n",
      "44           +3.5105408742225119e-02    1.48114305e-02    \n",
      "45           +3.4769898924539966e-02    9.95683329e-03    \n",
      "46           +3.4481096858530069e-02    4.55231506e-03    \n",
      "47           +3.4345269771415721e-02    5.98036872e-03    \n",
      "48           +3.4321969570466404e-02    9.97333410e-03    \n",
      "49           +3.4236156982700772e-02    8.41151634e-03    \n",
      "50           +3.4025980747864892e-02    3.52984235e-03    \n",
      "Terminated - max iterations reached after 163.58 seconds.\n",
      "\n",
      "(528, 100) (310, 100)\n",
      "(528, 4096) (310, 4096)\n",
      "(528, 100) (310, 100)\n",
      "(528, 4096) (310, 4096)\n",
      "[0.33774668]\n",
      "all acc [0.36513726]\n",
      "pseudo Xtu: [0.91294886]\n",
      "pseudo Xtk: [0.33774668]\n",
      "HOS: [0.49307835]\n",
      "[0.35389516]\n",
      "all acc [0.38144946]\n",
      "pseudo Xtu: [0.93253536]\n",
      "pseudo Xtk: [0.35389516]\n",
      "HOS: [0.51307824]\n",
      "[0.51078787]\n",
      "all acc [0.52232142]\n",
      "pseudo Xtu: [0.75299238]\n",
      "pseudo Xtk: [0.51078787]\n",
      "HOS: [0.60868078]\n",
      "[0.49290146]\n",
      "all acc [0.50285139]\n",
      "pseudo Xtu: [0.70184984]\n",
      "pseudo Xtk: [0.49290146]\n",
      "HOS: [0.57910431]\n",
      "[0.62593992]\n",
      "all acc [0.63561903]\n",
      "pseudo Xtu: [0.8292011]\n",
      "pseudo Xtk: [0.62593992]\n",
      "HOS: [0.71337426]\n",
      "[0.59220911]\n",
      "all acc [0.60124755]\n",
      "pseudo Xtu: [0.78201635]\n",
      "pseudo Xtk: [0.59220911]\n",
      "HOS: [0.67400469]\n"
     ]
    }
   ],
   "source": [
    "## 对齐源域数据\n",
    "Xs_all=np.vstack(Xs[0])\n",
    "Xs_all = scale(Xs_all, with_std=False)\n",
    "ys_all=np.hstack(ys[0])\n",
    "\n",
    "Xt_new=np.vstack(Xs[1])\n",
    "Xt_new = scale(Xt_new, with_std=False)\n",
    "Yt_pseudo=np.hstack(ys[1])\n",
    "\n",
    "np.random.seed(0)  # 使得PCA算法稳定\n",
    "pca = PCA(svd_solver=\"full\").fit(np.vstack((Xs_all,Xt_new)))\n",
    "W0 = pca.components_.T  # 初始化降维矩阵为Dxd维,D＞d\n",
    "Ws, Wt = W0, W0\n",
    "\n",
    "unique,index=np.unique(l,return_index=True)\n",
    "l_source=np.array(l[:index[-1]])\n",
    "W = cost_manifold(Ws, Wt,l_source, Cs, Xs_all, ys_all, Xt_new, Yt_pseudo, 50, 100).point\n",
    "Xs_pre = np.dot(Xs_all, W[0])\n",
    "Xt_pre = np.dot(Xt_new, W[1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xs_pre = np.dot(Xs_all, W[0])\n",
    "Xt_pre = np.dot(Xt_new, W[1])\n",
    "Xs_align=[Xs_pre,Xt_pre]\n",
    "print(Xs_pre.shape,Xt_pre.shape)\n",
    "print(Xs[0].shape,Xs[1].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_scale = scale(Xt, with_std=False)\n",
    "Xt_align=np.dot(Xt_scale, W[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pseudo_fuc_ms(Xs, ys, Xt, Yt,Cs,Cu,conf=0):\n",
    "    # 1.加载数据\n",
    "    Xs=np.vstack(Xs)\n",
    "    ys=np.hstack(ys)\n",
    "\n",
    "    Xs = scale(Xs, with_std=False)\n",
    "    Xt = scale(Xt, with_std=False)\n",
    "\n",
    "\n",
    "    # 2.设置分类模型,标注已知类伪标签\n",
    "    Xt_label, Xt_labelConf = cla_Svc(Xs, Xt, ys)\n",
    "\n",
    "    # conf = -0.25  # 根据SVM定义,置信度大于0才属于已知类\n",
    "    Xtk = Xt[Xt_labelConf >= conf]\n",
    "    Ytk = Yt[Xt_labelConf >= conf]\n",
    "    Ytk_pseudo = Xt_label[Xt_labelConf >= conf]\n",
    "    Xtu = Xt[Xt_labelConf < conf]\n",
    "    Ytu = Yt[Xt_labelConf < conf]\n",
    "    Ytu_pseudo = Xt_label[Xt_labelConf < conf]\n",
    "    Ytu_pseudo[:] = Cs\n",
    "    \n",
    "\n",
    "    # 5.目标域数据整合\n",
    "    Xt_new = np.vstack((Xtk, Xtu))\n",
    "    Yt_new = np.hstack((Ytk, Ytu))\n",
    "    Yt_pseudo = np.hstack((Ytk_pseudo, Ytu_pseudo))\n",
    "    \n",
    "\n",
    "    \n",
    "    acc,acc_unknown,acc_known,HOS=compute_acc(Yt_new,Yt_pseudo,Cs)\n",
    "    # acc_known=compute_acc_known(Yt_new[Yt_new < Cs],Yt_pseudo[Yt_new < Cs], Cs)\n",
    "    # one_unknown=compute_one_unknown(Yt_new[Yt_new >= Cs], Yt_pseudo[Yt_new >= Cs], Cs)\n",
    "    print('all acc',acc)\n",
    "    print(\"pseudo Xtu:\", acc_unknown)\n",
    "    print(\"pseudo Xtk:\", acc_known)\n",
    "    print('HOS:',HOS)\n",
    "\n",
    "    return Xt_new,Yt_new,Yt_pseudo\n",
    "\n",
    "\n",
    "def pseudo_fuc_ms_progressive(Xs, ys, Xt, Yt,Cs,k_conf=0,u_conf=0):\n",
    "    # 1.加载数据\n",
    "    Xs=np.vstack(Xs)\n",
    "    ys=np.hstack(ys)\n",
    "    # Xs=Xs[0]\n",
    "    # ys=ys[0]\n",
    "\n",
    "    Xs = scale(Xs, with_std=False)\n",
    "    Xt = scale(Xt, with_std=False)\n",
    "\n",
    "\n",
    "    # 2.设置分类模型,标注已知类伪标签\n",
    "    Xt_label, Xt_labelConf = cla_Svc(Xs, Xt, ys)\n",
    "\n",
    "    # conf = -0.25  # 根据SVM定义,置信度大于0才属于已知类\n",
    "    Xtk = Xt[Xt_labelConf >= k_conf]\n",
    "    Ytk = Yt[Xt_labelConf >= k_conf]\n",
    "    Ytk_pseudo = Xt_label[Xt_labelConf >= k_conf]\n",
    "\n",
    "    Xtu = Xt[Xt_labelConf < u_conf]\n",
    "    Ytu = Yt[Xt_labelConf < u_conf]\n",
    "    Ytu_pseudo = Xt_label[Xt_labelConf < u_conf]\n",
    "    Ytu_pseudo[:] = Cs\n",
    "    \n",
    "\n",
    "    # 5.目标域数据整合\n",
    "    Xt_new = np.vstack((Xtk, Xtu))\n",
    "    Yt_new = np.hstack((Ytk, Ytu))\n",
    "    Yt_pseudo = np.hstack((Ytk_pseudo, Ytu_pseudo))\n",
    "\n",
    "    #未分类样本整合\n",
    "    Xt_candidate = Xt[(Xt_labelConf < k_conf) & (Xt_labelConf >= u_conf)]\n",
    "    Yt_candidate = Yt[(Xt_labelConf < k_conf) & (Xt_labelConf >= u_conf)]\n",
    "    \n",
    "    acc,acc_unknown,acc_known,HOS=compute_acc(Yt_new,Yt_pseudo,Cs)\n",
    "    # acc_known=compute_acc_known(Yt_new[Yt_new < Cs],Yt_pseudo[Yt_new < Cs], Cs)\n",
    "    # one_unknown=compute_one_unknown(Yt_new[Yt_new >= Cs], Yt_pseudo[Yt_new >= Cs], Cs)\n",
    "    print('all acc',acc)\n",
    "    print(\"pseudo Xtu:\", acc_unknown)\n",
    "    print(\"pseudo Xtk:\", acc_known)\n",
    "    print('HOS:',HOS)\n",
    "\n",
    "    return Xt_new,Yt_new,Yt_pseudo,Xt_candidate,Yt_candidate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_new,Yt_new,Yt_pseudo=pseudo_fuc_ms(Xs_align,ys,Xt_align,yt,Cs,Cu,conf=-0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_new,Yt_new,Yt_pseudo=pseudo_fuc_ms(Xs,ys,Xt,yt,Cs,Cu,conf=-0.25)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_conf=-0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_new,Yt_new,Yt_pseudo,Xt_candidate,Yt_candidate=pseudo_fuc_ms_progressive(Xs_align,ys,Xt_align,yt,Cs,u_conf=u_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_new,Yt_new,Yt_pseudo,Xt_candidate,Yt_candidate=pseudo_fuc_ms_progressive(Xs,ys,Xt,yt,Cs,u_conf=u_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataAssign(Xs,ys,Xt,yt,Cs,split_num=2):\n",
    "    '''\n",
    "    置信度范围是-1->1  \n",
    "    '''\n",
    "    u_conf_step=1/split_num\n",
    "    u_conf=-1+u_conf_step\n",
    "    print(u_conf)\n",
    "    Xt_new,Yt_new,Yt_pseudo,Xt_candidate,Yt_candidate=pseudo_fuc_ms_progressive(Xs,ys,Xt,yt,Cs,u_conf=u_conf)\n",
    "    Xt_new[Yt_pseudo]\n",
    "    Xt_new2=np.vstack\n",
    "    Xt_new2,Yt_new2,Yt_pseudo2=pseudo_fuc_ms(Xs,ys,Xt_candidate,Yt_candidate,Cs,Cu,conf=-0.25)\n",
    "    return [Xt_new,Xt_new2],[Yt_new,Yt_new2],[Yt_pseudo,Yt_pseudo2]\n",
    "    \n",
    "Xt_news,Yt_news,Yt_pseudos=dataAssign(Xs,ys,Xt,yt,Cs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_new,Yt_new,Yt_pseudo,Xt_candidate,Yt_candidate=pseudo_fuc_ms_progressive(Xs,ys,Xt,yt,Cs,u_conf=u_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Xt_new,Yt_new,Yt_pseudo,Xt_candidate,Yt_candidate=pseudo_fuc_ms_progressive(Xs,ys,Xt,yt,Cs,u_conf=u_conf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.0 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "7561be136af4c20f61ae64066fbde9a25b93022926daecb6440711f63ea82dfb"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
